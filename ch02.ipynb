{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2941fcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "f = urlopen('http://hanbit.co.kr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3291f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c461309",
   "metadata": {},
   "outputs": [],
   "source": [
    "f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5560fa06",
   "metadata": {},
   "outputs": [],
   "source": [
    "f.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7feefdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "f.getheader('Content-Type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f88e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = f.info().get_content_charset(failobj='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b734213",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = f.read().decode('UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acfd81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60679dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from urllib.request import urlopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba133efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = urlopen('http://www.hanbit.co.kr/store/books/full_book_list.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caece37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = f.info().get_content_charset(failobj='utf-8')\n",
    "print('encoding:', encoding, file=sys.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5e2ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = f.read().decode(encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d1e514",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fb2c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "from urllib.request import urlopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436241d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = urlopen('http://www.hanbit.co.kr/store/books/full_book_list.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d709db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "bytes_content = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262da76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scanned_text = bytes_content[:1024].decode('ascii', errors='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a035a9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "match = re.search(r'charset=[\"\\']?([\\w-]+)', scanned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9824c2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if match:\n",
    "    encoding = match.group(1)\n",
    "else:\n",
    "    encoding = 'utf-8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f357e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('encoding:', encoding, file=sys.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afaab5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = bytes_content.decode(encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd218891",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a2c814",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"aa bb ab cb\"\n",
    "re.search(r'[acb]b', test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12e8f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154ccb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e98974",
   "metadata": {},
   "outputs": [],
   "source": [
    "re.search(r'a.*c', 'abc12DEF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55eda131",
   "metadata": {},
   "outputs": [],
   "source": [
    "re.search(r'a.*d', 'abc123DEF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f74593",
   "metadata": {},
   "outputs": [],
   "source": [
    "re.search(r'a.*d', 'abc123DEF', re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcb4116",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = re.search(r'a([A-Z])c([a-z])', 'aDcd3DEcf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d9a24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "m.group(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9057b9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall(r'\\w{3,}', 'This is a pen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c21c8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "re.sub(r'\\w{2,}', 'That', 'This is a pen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9172faf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from urllib.request import urlopen\n",
    "f = urlopen('http://www.hanbit.co.kr/store/books/full_book_list.html')\n",
    "\n",
    "encoding = f.info().get_content_charset(failobj=\"utf-8\")\n",
    "print('encoding: ', encoding, file=sys.stderr)\n",
    "text = f.read().decode(encoding)\n",
    "print(text)\n",
    "hf = open(\"./dp.html\", 'w', encoding='utf-8')\n",
    "\n",
    "for line in text:\n",
    "    hf.write(line)\n",
    "    \n",
    "hf.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a81d7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2~11 scraping_re.py 정규표현식으로 스크레핑하기\n",
    "import re\n",
    "from html import unescape\n",
    "\n",
    "with open('dp.html', encoding='utf-8') as f:\n",
    "    html = f.read()\n",
    "    \n",
    "#print(html)\n",
    "for partial_html in re.findall(r'<td class=\"left\"><a.*?</td>', html, re.DOTALL):\n",
    "    url = re.search(r'<a href=\"(.*?)\">', partial_html).group(1)\n",
    "    url = 'http://www.handbit.co.kr' + url\n",
    "    title = re.sub(r'<.*?>', '', partial_html)\n",
    "    title = unescape(title)\n",
    "    print('url:', url)\n",
    "    print('title:', title)\n",
    "    print('------'*7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61831219",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xml.etree import ElementTree\n",
    "\n",
    "tree = ElementTree.parse('rss.xml')\n",
    "\n",
    "root = tree.getroot()\n",
    "\n",
    "for item in root.findall('channel/item/description/body/location/data'):\n",
    "    tm_ef = item.find('tmEf').text\n",
    "    tmn = item.find('tmn').text\n",
    "    tmx = item.find('tmx').text\n",
    "    wf = item.find('wf').text\n",
    "    print(tm_ef, tmn, tmx, wf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8664b1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.15 save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460aae6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('top_cities.csv', 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['rank', 'city', 'population'])\n",
    "    writer.writerows([\n",
    "        [1, '상하이', 24150000],\n",
    "        [2, '카라치', 23500000],\n",
    "        [3, '베이징', 21516000],\n",
    "        [4, '텐진', 14722100],\n",
    "        [5, '이스탄불', 14160467],\n",
    "    ])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbe3867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.16\n",
    "import csv\n",
    "\n",
    "with open('top_cities2.csv', 'w', newline='') as f:\n",
    "    writer = csv.DictWriter(f, ['rank', 'city', 'population'])\n",
    "    writer.writeheader()\n",
    "    writer.writerows([\n",
    "        {'rank':1, 'city':'상하이', 'population':24150000},\n",
    "        {'rank':2, 'city':'카라치', 'population':23500000},\n",
    "        {'rank':3, 'city':'베이징', 'population':21516000},\n",
    "        {'rank':4, 'city':'텐진', 'population':14722100},\n",
    "        {'rank':5, 'city':'이스탄불', 'population':14160467},\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0e709c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "cities = [\n",
    "    {'rank':1, 'city':'상하이', 'population':24150000},\n",
    "    {'rank':2, 'city':'카라치', 'population':23500000},\n",
    "    {'rank':3, 'city':'베이징', 'population':21516000},\n",
    "    {'rank':4, 'city':'텐진', 'population':14722100},\n",
    "    {'rank':5, 'city':'이스탄불', 'population':14160467},\n",
    "]\n",
    "print(json.dumps(cities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09defe74",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(cities, ensure_ascii=False, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a04e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('top_cities.json', 'w') as f:\n",
    "    json.dump(cities, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3877649",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect('top_cities.db')\n",
    "c = conn.cursor()\n",
    "\n",
    "c.execute('DROP TABLE IF EXISTS cities')\n",
    "c.execute('''\n",
    "    CREATE TABLE cities (\n",
    "        rank integer,\n",
    "        city text,\n",
    "        population integer\n",
    "        )\n",
    "        ''')\n",
    "c.execute('INSERT INTO cities VALUES (?, ?, ?)', (1, '상하이', 24150000))\n",
    "c.execute('INSERT INTO cities VALUES (:rank, :city, :population)', \n",
    "          {'rank':2, 'city': '카라치', 'population': 23500000})\n",
    "\n",
    "c.executemany('INSERT INTO cities VALUES (:rank, :city, :population)', [\n",
    "    {'rank':3, 'city':'베이징', 'population':21516000},\n",
    "    {'rank':4, 'city':'텐진', 'population':14722100},\n",
    "    {'rank':5, 'city':'이스탄불', 'population':14160467},\n",
    "])\n",
    "\n",
    "conn.commit()\n",
    "\n",
    "c.execute('SELECT * FROM cities')\n",
    "for row in c.fetchall():\n",
    "    print(row)\n",
    "    \n",
    "conn.close"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042368d5",
   "metadata": {},
   "source": [
    "### 2.7 파이썬으로 스크레이핑하는 흐름\n",
    "    * fetch(url)\n",
    "       - 매개변수로 url을 받고 지정한 URL의 웹 페이지를 추출합니다.\n",
    "    * scrape(html)\n",
    "       - 매개변수로 html을 받고, 정규 표현식을 사용해 HTML에서 도서 정보를 추출합니다.\n",
    "    * save(db_path, books)\n",
    "       - 매개변수로 books라는 도서 목록을 받고, SQLite데이터베이스에 저장합니다.\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50af28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sqlite3\n",
    "from urllib.request import urlopen\n",
    "from html import unescape\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    메인 처리입니다.\n",
    "    fetch(), scrape(), save() 함수를 호출합니다.\n",
    "    \"\"\"\n",
    "    html = fetch('http://www.hanbit.co.kr/store/books/full_book_list.html')\n",
    "    books = scrape('html')\n",
    "    save('books.db', books)\n",
    "\n",
    "def fetch(url):\n",
    "    \"\"\"\n",
    "    매개변수로 전달받을 url을 기반으로 웹 페이지를 추출합니다.\n",
    "    웹 페이지의 인코딩 형식은 Content-Type 헤더를 기반으로 알아냅니다.\n",
    "    반환값: str 자료형의 HTML\n",
    "    \"\"\"\n",
    "    f = urlopen(url)\n",
    "    encoding = f.info().get_content_charset(failobj=\"utf-8\")\n",
    "    html = f.read().decode(encoding)\n",
    "    return html\n",
    "\n",
    "def scrape(html):\n",
    "    \"\"\"\n",
    "    매개변수 html로 받은 HTML을 기반으로 정규 표현식을 사용해 도서 정보를 추출합니다.\n",
    "    반환값: 도서(dict) 리스트\n",
    "    \"\"\"\n",
    "    books = []\n",
    "    for partial_html in re.findall(r'<td class=\"left\"><a.*?</td>', html, re.DOTALL):\n",
    "        url = re.search(r'<a href=\"(.*?)\">', partial_html).group(1)\n",
    "        url = 'http://www.hanbit.co.kr' + url\n",
    "        \n",
    "        title = re.sub(r'<.*?>', '', partial_html)\n",
    "        title = unescape(title)\n",
    "        books.append({'url': url, 'title':title})\n",
    "    return books\n",
    "\n",
    "def save(db_path, books):\n",
    "    \"\"\"\n",
    "    매개변수 books로 전달된 도서 목록을 SQLite 데이터베이스에 저장합니다.\n",
    "    데이터베이스의 경로는 매개변수 dp_path로 지정합니다.\n",
    "    반환값: None(없음)\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    c = conn.cursor()\n",
    "    c.execute('DROP TABLE IF EXISTS books')\n",
    "    c.execute('''\n",
    "        CREATE TABLE books (\n",
    "            title text,\n",
    "            url text\n",
    "        )\n",
    "    ''')\n",
    "    c.executemany('INSERT INTO books VALUES (:title, :url)', books)\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    \n",
    "main()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a984d9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sqlite3\n",
    "from urllib.request import urlopen\n",
    "from html import unescape\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    메인 처리입니다.\n",
    "    fetch(), scrape(), save() 함수를 호출합니다.\n",
    "    \"\"\"\n",
    "    html = fetch('http://www.hanbit.co.kr/store/books/full_book_list.html')\n",
    "    books = scrape(html)\n",
    "    save('books.db', books)\n",
    "\n",
    "def fetch(url):\n",
    "    \"\"\"\n",
    "    매개변수로 전달받을 url을 기반으로 웹 페이지를 추출합니다.\n",
    "    웹 페이지의 인코딩 형식은 Content-Type 헤더를 기반으로 알아냅니다.\n",
    "    반환값: str 자료형의 HTML\n",
    "    \"\"\"\n",
    "    f = urlopen(url)\n",
    "    # HTTP 헤더를 기반으로 인코딩 형식을 추출합니다.\n",
    "    encoding = f.info().get_content_charset(failobj=\"utf-8\")\n",
    "    # 추출한 인코딩 형식을 기반으로 문자열을 디코딩합니다.\n",
    "    html = f.read().decode(encoding)\n",
    "    return html\n",
    "\n",
    "def scrape(html):\n",
    "    \"\"\"\n",
    "    매개변수 html로 받은 HTML을 기반으로 정규 표현식을 사용해 도서 정보를 추출합니다.\n",
    "    반환값: 도서(dict) 리스트\n",
    "    \"\"\"\n",
    "    books = []\n",
    "    # re.findall()을 사용해 도서 하나에 해당하는 HTML을 추출합니다.\n",
    "    for partial_html in re.findall(r'<td class=\"left\"><a.*?</td>', html, re.DOTALL):\n",
    "        # 도서의 URL을 추출합니다.\n",
    "        url = re.search(r'<a href=\"(.*?)\">', partial_html).group(1)\n",
    "        url = 'http://www.hanbit.co.kr' + url\n",
    "        # 태그를 제거해서 도서의 제목을 추출합니다.\n",
    "        title = re.sub(r'<.*?>', '', partial_html)\n",
    "        title = unescape(title)\n",
    "        books.append({'url': url, 'title': title})\n",
    "    \n",
    "    return books\n",
    "\n",
    "def save(db_path, books):\n",
    "    \"\"\"\n",
    "    매개변수 books로 전달된 도서 목록을 SQLite 데이터베이스에 저장합니다.\n",
    "    데이터베이스의 경로는 매개변수 dp_path로 지정합니다.\n",
    "    반환값: None(없음)\n",
    "    \"\"\"\n",
    "    # 데이터베이스를 열고 연결을 확립합니다.\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    # 커서를 추출합니다.\n",
    "    c = conn.cursor()\n",
    "    # execute() 메서드로 SQL을 실행합니다.\n",
    "    # 스크립트를 여러 번 실행할 수 있으므로 기존의 books 테이블을 제거합니다.\n",
    "    c.execute('DROP TABLE IF EXISTS books')\n",
    "    # books 테이블을 생성합니다.\n",
    "    c.execute('''\n",
    "        CREATE TABLE books (\n",
    "            title text,\n",
    "            url text\n",
    "        )\n",
    "    ''')\n",
    "    # executemany() 메서드를 사용하면 매개변수로 리스트를 지정할 수 있습니다.\n",
    "    c.executemany('INSERT INTO books VALUES (:title, :url)', books)\n",
    "    # 변경사항을 커밋(저장)합니다.\n",
    "    conn.commit()\n",
    "    # 연결을 종료합니다.\n",
    "    conn.close()\n",
    "\n",
    "# python 명령어로 실행한 경우 main() 함수를 호출합니다.\n",
    "# 이는 모듈로써 다른 파일에서 읽어 들였을 때 main() 함수가 호출되지 않게 하는 것입니다.\n",
    "# 파이썬 프로그램의 일반적인 작성 방식입니다.\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b76e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4209edbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b8ce28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d6381f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "r = requests.get('http://hanbit.co.kr')\n",
    "type(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5e6f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "r.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b744fce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "r.headers['content-type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912edd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "r.encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d912d8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "r.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbfc4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get('https://jsonplaceholder.typicode.com/todos/1')\n",
    "r.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f976b6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018870ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.post('http://httpbin.org/post', data={'key1':'value'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef672754",
   "metadata": {},
   "outputs": [],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcded1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lxml.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1b0ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = lxml.html.parse('full_book_list.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b82ac8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f36ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "html = tree.getroot()\n",
    "\n",
    "for a in html.xpath('//a'):\n",
    "    print(a.get('href'), a.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e2c084",
   "metadata": {},
   "outputs": [],
   "source": [
    "html = lxml.html.fromstring('''\n",
    "<html>\n",
    "    <head><title>온라인 과일 가게</title></head>\n",
    "    <body>\n",
    "        <h1 id=\"main\">오늘의 과일</h1>\n",
    "        <ul>\n",
    "            <li>사과</li>\n",
    "            <li class=\"featured\">귤</li>\n",
    "            <li>포도</li>\n",
    "        </ul>\n",
    "    </body>\n",
    "</html>\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359b8213",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56100627",
   "metadata": {},
   "outputs": [],
   "source": [
    "html.xpath('//li')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70e7f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in html.xpath('//li'):\n",
    "    print(a.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1003f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "h1 = html.xpath('//h1')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84c88a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "h1.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3157dd9a",
   "metadata": {},
   "source": [
    "h1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcd7c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "h1.get('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe6e68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "h1.attrib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f8221e",
   "metadata": {},
   "source": [
    "## 웹 서버에 요청하고 응답받기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f958365b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "url = \"https://www.python.org/\"\n",
    "resp = requests.get(url)\n",
    "print(resp)\n",
    "\n",
    "url2 = \"https://www.python.org/1\"\n",
    "resp = requests.get(url2)\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0702753b",
   "metadata": {},
   "source": [
    "## 웹 페이지 소스코드 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c277064",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "url = \"https://www.python.org\"\n",
    "resp = requests.get(url)\n",
    "print(resp.__dir__())\n",
    "print('---'*20)\n",
    "print(resp.headers)\n",
    "print('---'*20)\n",
    "print(resp.status_code)\n",
    "print('---'*20)\n",
    "#print(resp._content)\n",
    "print('---'*20)\n",
    "print(resp.url)\n",
    "print('---'*20)\n",
    "print(resp.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfed768",
   "metadata": {},
   "source": [
    "/robots.txt->사이트 크롤링 허용 부분 정보 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827350bc",
   "metadata": {},
   "source": [
    "## 로봇 배제 표준(robots.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1168e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "urls = ['https://www.naver.com/', 'https://www.python.org/']\n",
    "filename = 'robots.txt'\n",
    "\n",
    "for url in urls:\n",
    "    file_path = url + filename\n",
    "    resp = requests.get(file_path)\n",
    "    #print(file_path)\n",
    "    print(resp.text)\n",
    "    print('---'*20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a19263a",
   "metadata": {},
   "source": [
    "## Beautifulsoup 객체만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6098c84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02828fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup('<b class=\"boldest\">Extremely bold</b>')\n",
    "tag = soup.b\n",
    "tag['class']\n",
    "tag.attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da526c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://en.wikipedia.org/wiki/Seoul_Metropolitan_Subway'\n",
    "resp = requests.get(url)\n",
    "html_src = resp.text\n",
    "#print(html_src)\n",
    "soup = BeautifulSoup(html_src, 'html.parser')\n",
    "print('title 태그 요소: ', soup.title)\n",
    "print('title 태그 이름: ', soup.title.name)\n",
    "print('title 태그 문자열: ', soup.title.string)\n",
    "first_img = soup.select('div > table > tbody > tr > td > a > img')\n",
    "print(first_img)\n",
    "print('---'*20)\n",
    "first_img2 = soup.find(name='img', attrs={'alt':'Seoul-metro-2009-20180916-103548.jpg'})\n",
    "print(first_img2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f11687",
   "metadata": {},
   "source": [
    "## 웹 문서의 그림 이미지 파일을 PC에 저장하기-완료. 다운로드 잘 됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dec2bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://en.wikipedia.org/wiki/Seoul_Metropolitan_Subway'\n",
    "resp = requests.get(url)\n",
    "html_src = resp.text\n",
    "\n",
    "soup = BeautifulSoup(html_src, 'html.parser')\n",
    "\n",
    "target_image = soup.find(name='img', attrs={'alt': 'Seoul-metro-2009-20180916-103548.jpg'})\n",
    "print('HTML 요소: ', target_image)\n",
    "print('--'*25)\n",
    "targ_img_src = target_image.get('src')\n",
    "print('이미지 파일 경로: ', targ_img_src)\n",
    "print('--'*25)\n",
    "target_img_resp = requests.get('http:'+ targ_img_src)\n",
    "print(target_img_resp)\n",
    "print('--'*20)\n",
    "out_file_path = \"download_image.jpg\"\n",
    "\n",
    "with open(out_file_path, 'wb') as out_file:\n",
    "    out_file.write(target_img_resp.content)\n",
    "    print('이미지 파일로 저장하였습니다.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8d84b6",
   "metadata": {},
   "source": [
    "## 웹 문서에 포함된 모든 하이퍼링크 추출하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe35002",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, re\n",
    "from bs4 import BeautifulSoup\n",
    "url = 'https://en.wikipedia.org/wiki/Seoul_Metropolitan_Subway'\n",
    "resp = requests.get(url)\n",
    "html_src = resp.text\n",
    "soup = BeautifulSoup(html_src, 'html.parser')\n",
    "\n",
    "links = soup.find_all(\"a\")\n",
    "print(type(links))\n",
    "print(\"하이퍼링크의 개수: \", len(links))\n",
    "print('--'*20)\n",
    "print('첫 3개의 원소: ', links[:3])\n",
    "print('--'*20)\n",
    "wiki_links = soup.find_all(name='a', href=re.compile(\"/wiki/\"), limit=3)\n",
    "print('/wiki/문자열이 포함된 하이퍼링크: ', wiki_links)\n",
    "print('--'*20)\n",
    "external_links = soup.find_all(name='a', attrs={'class':\"external text\"}, limit=3)\n",
    "print('class 속성으로 추출한 하이퍼링크: ', external_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452b4346",
   "metadata": {},
   "source": [
    "## CSS Selector 활용하기 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70ddd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://en.wikipedia.org/wiki/Seoul_Metropolitan_Subway'\n",
    "resp = requests.get(url)\n",
    "html_src = resp.text\n",
    "\n",
    "soup = BeautifulSoup(html_src, 'html.parser')\n",
    "\n",
    "subway_image = soup.select('div.mw-parser-output > table > tbody > tr > td > a > img')\n",
    "subway_image1 = soup.select('#mw-content-text > div.mw-parser-output > table:nth-child(5) > tbody > tr:nth-child(2) > td > a > img')\n",
    "# #mw-content-text > div.mw-parser-output > table:nth-child(5) > tbody > tr:nth-child(2) > td > a > img\n",
    "# div.mw-parser-output > table > tbody > tr > td > a > img\n",
    "print(subway_image)\n",
    "print('--'*20)\n",
    "#print(subway_image1[0])\n",
    "\n",
    "subway_image2 = soup.select('a > img')\n",
    "print(len(subway_image2))\n",
    "\n",
    "for i in range(5):\n",
    "    print(subway_image2[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1340d08f",
   "metadata": {},
   "source": [
    "## CSS Selector 활용하기 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6764a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://en.wikipedia.org/wiki/Seoul_Metropolitan_Subway'\n",
    "resp = requests.get(url)\n",
    "html_src = resp.text\n",
    "soup = BeautifulSoup(html_src, 'html.parser')\n",
    "\n",
    "links = soup.select('a')\n",
    "print(len(links))\n",
    "print()\n",
    "print(links[:3])\n",
    "print()\n",
    "external_links = soup.select('a[class=\"external text\"]')\n",
    "print(external_links[:3])\n",
    "print()\n",
    "\n",
    "external_links = soup.select('a.external text')\n",
    "#빈공간은 인식 못함\n",
    "print(external_links[:3])\n",
    "print()\n",
    "\n",
    "jump_links = soup.select('a[class=\"mw-jump-link\"]')\n",
    "print(jump_links[:3])\n",
    "print()\n",
    "\n",
    "id_selector = soup.select('#siteNotice')\n",
    "print('id_selector: ', id_selector)\n",
    "print()\n",
    "id_selector2 = soup.select('div#siteNotice')\n",
    "print('id_selector2: ', id_selector2)\n",
    "print()\n",
    "class_selector = soup.select('span.mw-headline')\n",
    "print('class_selector: ', class_selector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27afc1d1",
   "metadata": {},
   "source": [
    "## 구글 뉴스 클리핑하기1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818b2d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "# keyword_input = \"아파트\"\n",
    "\n",
    "# keyword = urllib.parse.quote(keyword_input)\n",
    "# print('파이썬 문자열을 URL 코드로 변환: ', keyword)\n",
    "\n",
    "base_url = \"https://news.google.com\"\n",
    "# search_url = base_url + \"/search?q=\" + keyword +\"&hl=ko&gl=KR&ceid=KR%3Ako\"\n",
    "# print(\"검색어와 조합된 URL: \", search_url)\n",
    "\n",
    "# resp = requests.get(search_url)\n",
    "# html_src = resp.text\n",
    "# soup = BeautifulSoup(html_src, 'html.parser')\n",
    "# #print(soup)\n",
    "\n",
    "\n",
    "# news_items = soup.select('div[class=\"xrnccd\"]')\n",
    "\n",
    "# #print(len(news_items))\n",
    "#n = 0\n",
    "#for i in news_items:\n",
    "    #n += 1\n",
    "    #print(f'{n}번째: {i}')\n",
    "    #print()\n",
    "n = 0    \n",
    "# for item in news_items:\n",
    "#     n += 1\n",
    "#     news_title = item.find('a', attrs={'class': 'DY5T1d'}).getText()\n",
    "#     print(f'{n}번째: {news_title}')\n",
    "#     news_agency = item.find('a', attrs={'class': 'wEwyrc'}).getText()\n",
    "#     print(news_agency)\n",
    "    \n",
    "#     news_reporting = item.find('time', attrs={'class': 'WW6dff'}).text\n",
    "#     print(news_reporting)\n",
    "    \n",
    "#     news_reporting = item.find('time', attrs={'class': 'WW6dff'})\n",
    "#     news_reporting_datetime = news_reporting.get('datetime').split('T')\n",
    "#     news_reporting_date = news_reporting_datetime[0]\n",
    "#     news_reporting_time = news_reporting_datetime[1][:-1]\n",
    "#     print(news_reporting_date, news_reporting_time)\n",
    "#     print()\n",
    "\n",
    "def google_news_clipping(keyword_input, limit=5):\n",
    "    \n",
    "    keyword = urllib.parse.quote(keyword_input)\n",
    "\n",
    "    url = base_url + \"/search?q=\" + keyword +\"&hl=ko&gl=KR&ceid=KR%3Ako\"\n",
    "    \n",
    "    resp = requests.get(url)\n",
    "    html_src = resp.text\n",
    "    soup = BeautifulSoup(html_src, 'html.parser')\n",
    "    \n",
    "    news_items = soup.select('div[class=\"xrnccd\"]')\n",
    "    \n",
    "    links = []; titles=[]; agencies=[]; reporting_dates=[]; reporting_times=[];\n",
    "    \n",
    "    for item in news_items[:limit]:\n",
    "        link = item.find('a', attrs={'class': 'VDXfz'}).get('href')\n",
    "        news_link = base_url + link[1:]\n",
    "        links.append(news_link)\n",
    "        \n",
    "        news_title = item.find('a', attrs={'class': 'DY5T1d'}).getText()\n",
    "        titles.append(news_title)\n",
    "        \n",
    "        news_agency = item.find('a', attrs={'class': 'wEwyrc'}).text\n",
    "        agencies.append(news_agency)\n",
    "        \n",
    "        news_reporting = item.find('time', attrs={'class':'WW6dff'})\n",
    "        news_reporting_datetime = news_reporting.get('datetime').split('T')\n",
    "        \n",
    "        news_reporting_date = news_reporting_datetime[0]\n",
    "        news_reporting_time = news_reporting_datetime[1][:-1]\n",
    "        reporting_dates.append(news_reporting_date)\n",
    "        reporting_times.append(news_reporting_time)\n",
    "    \n",
    "    result = {'link': links, 'title':titles, 'agency':agencies,\\\n",
    "             'date': reporting_dates, 'time': reporting_times}\n",
    "    \n",
    "    return result\n",
    "\n",
    "def display(news, num):\n",
    "    \n",
    "    for i in range(num):\n",
    "        print(news['title'][i])\n",
    "        print(news['agency'][i])\n",
    "        print(news['date'][i])\n",
    "        print(news['time'][i])\n",
    "        print(news['link'][i])\n",
    "        print()\n",
    "        print()\n",
    "\n",
    "search_word = input(\"검색어를 입력하세요: \")\n",
    "# print(\"search_word: \", search_word)\n",
    "        \n",
    "news = google_news_clipping(search_word, 5)\n",
    "display(news,5)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fedae4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://news.google.com/search?q=%ED%8C%8C%EC%9D%B4%EC%8D%AC&hl=ko&gl=KR&ceid=KR%3Ako"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11e9d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#yDmH0d > c-wiz:nth-child(31) > div > div.FVeGwb.CVnAc.Haq2Hf.bWfURe > div.ajwQHc.BL5WZb.RELBvb > div > main > c-wiz > div.lBwEZb.BL5WZb.xP6mwf > div.NiLAwe.mi8Lec.gAl5If.jVwmLb.Oc0wGc.R7GTQ.keNKEd.j7vNaf.nID9nc > div > div > article > h3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e2d1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "신문사: #yDmH0d > c-wiz:nth-child(31) > div > div.FVeGwb.CVnAc.Haq2Hf.bWfURe > div.ajwQHc.BL5WZb.RELBvb > div > main > c-wiz > div.lBwEZb.BL5WZb.xP6mwf > div.NiLAwe.mi8Lec.gAl5If.jVwmLb.Oc0wGc.R7GTQ.keNKEd.j7vNaf.nID9nc > div > div > article > div > div > a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35005df4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "====== WebDriver manager ======\n",
      "Current google-chrome version is 100.0.4896\n",
      "Get LATEST chromedriver version for 100.0.4896 google-chrome\n",
      "Driver [C:\\Users\\PNU\\.wdm\\drivers\\chromedriver\\win32\\100.0.4896.60\\chromedriver.exe] found in cache\n",
      "C:\\Users\\PNU\\AppData\\Local\\Temp/ipykernel_2328/3837665581.py:12: DeprecationWarning: find_element_by_css_selector is deprecated. Please use find_element(by=By.CSS_SELECTOR, value=css_selector) instead\n",
      "  login = driver.find_element_by_css_selector('#danawa_header > div > div > div.main-header__banner > div.main-header__user > div:nth-child(5) > a')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTML 요소:  <selenium.webdriver.remote.webelement.WebElement (session=\"fd007584a05489f1d47cb8fb7abbab4a\", element=\"6c5eaf9a-f04d-42c7-8b0f-521da2a5762f\")>\n",
      "태그 이름:  a\n",
      "문자열:  로그인\n",
      "href 속성:  https://auth.danawa.com/login?url=http%3A%2F%2Fwww.danawa.com%2F\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PNU\\AppData\\Local\\Temp/ipykernel_2328/3837665581.py:33: DeprecationWarning: find_element_by_* commands are deprecated. Please use find_element() instead\n",
      "  driver.find_element_by_id('danawa-member-login-input-id').send_keys(my_id)\n",
      "C:\\Users\\PNU\\AppData\\Local\\Temp/ipykernel_2328/3837665581.py:35: DeprecationWarning: find_element_by_name is deprecated. Please use find_element(by=By.NAME, value=name) instead\n",
      "  driver.find_element_by_name('password').send_keys(my_pw)\n",
      "C:\\Users\\PNU\\AppData\\Local\\Temp/ipykernel_2328/3837665581.py:36: DeprecationWarning: find_element_by_css_selector is deprecated. Please use find_element(by=By.CSS_SELECTOR, value=css_selector) instead\n",
      "  driver.find_element_by_css_selector('button.btn_login').click()\n",
      "C:\\Users\\PNU\\AppData\\Local\\Temp/ipykernel_2328/3837665581.py:39: DeprecationWarning: find_element_by_css_selector is deprecated. Please use find_element(by=By.CSS_SELECTOR, value=css_selector) instead\n",
      "  wishlist = driver.find_element_by_css_selector('#danawa_header > div > div > div.main-header__banner > div.main-header__user > div:nth-child(4) > a')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title:  삼성전자 비스포크 RF85A911155\n",
      "price:  ['1,684,850원']\n",
      "link:  http://prod.danawa.com/info/?pcode=14710061\n",
      "title:  QCY T13 (정품)\n",
      "price:  ['14,410원']\n",
      "link:  http://prod.danawa.com/info/?pcode=15071006\n",
      "title:  LG전자 오브제컬렉션 코드제로 ThinQ A9S AO9571 (카밍베이지)\n",
      "price:  ['991,160원']\n",
      "link:  http://prod.danawa.com/info/?pcode=13327520\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "driver.implicitly_wait(3)\n",
    "driver.get(\"https://www.danawa.com\")\n",
    "\n",
    "login = driver.find_element_by_css_selector('#danawa_header > div > div > div.main-header__banner > div.main-header__user > div:nth-child(5) > a')\n",
    "#danawa_header > div > div > div.main-header__banner > div.main-header__user > div:nth-child(5) > a\n",
    "\n",
    "print('HTML 요소: ', login)\n",
    "print('태그 이름: ', login.tag_name)\n",
    "print('문자열: ', login.text)\n",
    "print('href 속성: ', login.get_attribute('href'))\n",
    "driver.implicitly_wait(5)\n",
    "login.click()\n",
    "\n",
    "\n",
    "driver.implicitly_wait(10)\n",
    "# naver_login = driver.find_element_by_css_selector('#danawa-member-login-snsButton-naver')\n",
    "# driver.implicitly_wait(3)\n",
    "\n",
    "# naver_login.click()\n",
    "\n",
    "my_id = \"yjk1322\"\n",
    "my_pw = \"alcjsdbwls4132*\"\n",
    "\n",
    "\n",
    "driver.find_element_by_id('danawa-member-login-input-id').send_keys(my_id)\n",
    "driver.implicitly_wait(3)\n",
    "driver.find_element_by_name('password').send_keys(my_pw)\n",
    "driver.find_element_by_css_selector('button.btn_login').click()\n",
    "\n",
    "\n",
    "wishlist = driver.find_element_by_css_selector('#danawa_header > div > div > div.main-header__banner > div.main-header__user > div:nth-child(4) > a')\n",
    "wishlist.click()\n",
    "driver.implicitly_wait(2)\n",
    "html_src = driver.page_source\n",
    "# print(html_src)\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "soup = BeautifulSoup(html_src, 'html.parser')\n",
    "\n",
    "\n",
    "wish_table = soup.select('table[class=\"tbl wish_tbl\"]')[0]\n",
    "# print(wish_table)\n",
    "# print(type(wish_table))\n",
    "wish_items = wish_table.select('tbody tr')\n",
    "# print(wish_items)\n",
    "\n",
    "for item in wish_items:\n",
    "    title = item.find('div', {'class': 'tit'}).text\n",
    "    price = item.find('span', {'class': 'price'}).text\n",
    "    link = item.find('a', href=re.compile(\"http://prod.danawa.com/info/\")).get('href')\n",
    "    print('title: ', title)\n",
    "    print('price: ', price.split())\n",
    "    print('link: ', link)\n",
    "    print('--'*20)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f51a9185",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "====== WebDriver manager ======\n",
      "Current google-chrome version is 100.0.4896\n",
      "Get LATEST chromedriver version for 100.0.4896 google-chrome\n",
      "Driver [C:\\Users\\PNU\\.wdm\\drivers\\chromedriver\\win32\\100.0.4896.60\\chromedriver.exe] found in cache\n",
      "C:\\Users\\PNU\\AppData\\Local\\Temp/ipykernel_2328/2895235658.py:13: DeprecationWarning: find_element_by_css_selector is deprecated. Please use find_element(by=By.CSS_SELECTOR, value=css_selector) instead\n",
      "  excel_download = driver.find_element_by_css_selector('img[alt=\"download\"]')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파일 다운로드 실행...\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "driver.implicitly_wait(30)\n",
    "driver.get(\"https://ecos.bok.or.kr/jsp/vis/keystat/#/key\")\n",
    "\n",
    "excel_download = driver.find_element_by_css_selector('img[alt=\"download\"]')\n",
    "driver.implicitly_wait(3)\n",
    "excel_download.click()\n",
    "time.sleep(5)\n",
    "print(\"파일 다운로드 실행...\")\n",
    "# driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730c89c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69a9506",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install webdriver_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b30c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install nbconvert, pandoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f39462e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<module 're' from 'C:\\\\Anaconda3\\\\lib\\\\re.py'>\n"
     ]
    }
   ],
   "source": [
    "import re; import sys; print(sys.modules['re'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58067abc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b296d04d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
